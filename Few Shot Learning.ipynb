{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import re,os,pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = api.load('glove-twitter-25')\n",
    "# pickle.dump(model,open('glove-twitter-25.pkl','wb'))\n",
    "model2=pickle.load(open('glove-twitter-25.pkl','rb'))\n",
    "# model2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dataset  length :  1345\n",
      "        Unnamed: 0  bin_cat\n",
      "count  1345.000000   1345.0\n",
      "mean    672.000000      0.0\n",
      "std     388.412367      0.0\n",
      "min       0.000000      0.0\n",
      "25%     336.000000      0.0\n",
      "50%     672.000000      0.0\n",
      "75%    1008.000000      0.0\n",
      "max    1344.000000      0.0\n",
      "   Unnamed: 0 place                                               text  \\\n",
      "0           0  None                                                Hya   \n",
      "1           1  None   Prasid ager group ta r admin R keu na.. so ok...   \n",
      "2           2  None   Hmm Kalo Dal Nei Ki levelr stalk korish 😓 Ke ...   \n",
      "3           3  None   Ok Bnara bol Sudeshnar sathe ki korechis Ki k...   \n",
      "4           4  None   Baaltai Dekhabi na to ? Bujhe gechi Mondarmon...   \n",
      "\n",
      "   category  bin_cat  \n",
      "0  chatting        0  \n",
      "1  chatting        0  \n",
      "2  chatting        0  \n",
      "3  chatting        0  \n",
      "4  chatting        0  \n",
      " dataset 2 length :  77\n",
      "       Unnamed: 0  bin_cat\n",
      "count   77.000000     77.0\n",
      "mean    38.000000      0.0\n",
      "std     22.371857      0.0\n",
      "min      0.000000      0.0\n",
      "25%     19.000000      0.0\n",
      "50%     38.000000      0.0\n",
      "75%     57.000000      0.0\n",
      "max     76.000000      0.0\n",
      "   Unnamed: 0 place                                               text  \\\n",
      "0           0  None                    ‎Vishnu Gaud created this group   \n",
      "1           1  None   ‎You were added <‎image omitted> <‎image omit...   \n",
      "2           2  None   Fancy dress competition? 😂😂😂 ‎Messages you se...   \n",
      "3           3  None   😀 😄😄😄😃😃 હરીવંશરાય બચ્ચન.....मै यादों का किस्स...   \n",
      "4           4  None  ....मै देर रात तक जागूँ तो ,कुछ दोस्त बहुत याद...   \n",
      "\n",
      "   category  bin_cat  \n",
      "0  chatting        0  \n",
      "1  chatting        0  \n",
      "2  chatting        0  \n",
      "3  chatting        0  \n",
      "4  chatting        0  \n",
      " dataset 3 length :  655\n",
      "       Unnamed: 0  bin_cat\n",
      "count  653.000000    653.0\n",
      "mean   327.520674      0.0\n",
      "std    189.281743      0.0\n",
      "min      0.000000      0.0\n",
      "25%    165.000000      0.0\n",
      "50%    328.000000      0.0\n",
      "75%    491.000000      0.0\n",
      "max    654.000000      0.0\n",
      "   Unnamed: 0 place                                               text  \\\n",
      "0           0  None                                               text   \n",
      "1           1  None  [START]\"Hi there\"Oh😄 here is afternoon!How do ...   \n",
      "2           2  None  \"I've talked with 143 users\"Oh\"I started chatt...   \n",
      "3           3  None  You asked about the bunnies. I haven't seen an...   \n",
      "4           4  None  \"I don't count my readings in words\"Thanks.I t...   \n",
      "\n",
      "   category  bin_cat  \n",
      "0  chatting        0  \n",
      "1  chatting        0  \n",
      "2  chatting        0  \n",
      "3  chatting        0  \n",
      "4  chatting        0  \n",
      " dataset 4 length :  5306\n",
      "        Unnamed: 0  bin_cat\n",
      "count  5291.000000   5291.0\n",
      "mean   2657.618976      1.0\n",
      "std    1535.481725      0.0\n",
      "min       0.000000      1.0\n",
      "25%    1328.500000      1.0\n",
      "50%    2657.000000      1.0\n",
      "75%    3986.500000      1.0\n",
      "max    5317.000000      1.0\n",
      "   Unnamed: 0               place  \\\n",
      "0           0                None   \n",
      "1           1           NEW DELHI   \n",
      "2           2  THIRUVANANTHAPURAM   \n",
      "3           3              NAGPUR   \n",
      "4           4              Panaji   \n",
      "\n",
      "                                                text category  bin_cat  \n",
      "0  [\"After the critically acclaimed Njan Steve Lo...     fake        1  \n",
      "1   Your   card could soon become a universal pay...     fake        1  \n",
      "2   As per latest weather forecast, the depressio...     fake        1  \n",
      "3   Controversial lawyer Satish Uke is set to fac...     fake        1  \n",
      "4   Announced about three years ago, the project ...     fake        1  \n",
      " dataset 5 length :  7000\n",
      "         Unnamed: 0  bin_cat\n",
      "count  6.998000e+03   6998.0\n",
      "mean   5.023169e+06      0.0\n",
      "std    5.081230e+06      0.0\n",
      "min    0.000000e+00      0.0\n",
      "25%    5.158675e+05      0.0\n",
      "50%    3.070428e+06      0.0\n",
      "75%    8.684865e+06      0.0\n",
      "max    1.616752e+07      0.0\n",
      "   Unnamed: 0 place                                              title  \\\n",
      "0           0  None   Children's Day Special: Shashi Tharoor's book...   \n",
      "1           1  None   Rajasthan Assembly Election 2018: Congress' S...   \n",
      "2           3  None   Rajasthan Assembly Election 2018: Congress fi...   \n",
      "3           6  None   MP Assembly Election 2018: BJP expels 53 rebe...   \n",
      "4          10  None   MP Assembly Election 2018: Finance Minister A...   \n",
      "\n",
      "                                                text      category  bin_cat  \n",
      "0  A trailer of the series has also been released...  General News        0  \n",
      "1  “ “As the Rajasthan Assembly election is aroun...  General News        0  \n",
      "2  The 54-year-old Rajasthan lawmaker Jaswant Sin...  General News        0  \n",
      "3  As the Madhya Pradesh Assembly election is aro...  General News        0  \n",
      "4  \"Only those who have a lot to hide have taken ...  General News        0  \n",
      " dataset 6 length :  1507\n",
      "        Unnamed: 0  bin_cat\n",
      "count  1313.000000   1313.0\n",
      "mean    742.802742      1.0\n",
      "std     442.219864      0.0\n",
      "min       0.000000      1.0\n",
      "25%     356.000000      1.0\n",
      "50%     737.000000      1.0\n",
      "75%    1127.000000      1.0\n",
      "max    1506.000000      1.0\n",
      "   Unnamed: 0                                              title  \\\n",
      "0           0  Was Srinagar’s Shankaracharya Temple lit up fo...   \n",
      "1           1  Clipped video shared to claim Harsh Mander ins...   \n",
      "2           2  AAP offers monetary relief to only Muslim vict...   \n",
      "3           3  Old images from Syria, Bangladesh shared as De...   \n",
      "4           4  Video of infant and woman’s dead bodies dug ou...   \n",
      "\n",
      "                                                link            date  \\\n",
      "0  https://www.altnews.in/was-srinagars-shankarac...  5th March 2020   \n",
      "1  https://www.altnews.in/clipped-video-shared-to...  4th March 2020   \n",
      "2  https://www.altnews.in/aap-offers-monetary-rel...  4th March 2020   \n",
      "3  https://www.altnews.in/old-images-from-syria-b...  4th March 2020   \n",
      "4  https://www.altnews.in/video-of-woman-and-a-ba...  4th March 2020   \n",
      "\n",
      "                                                text category  bin_cat  \n",
      "0  “After decades of darkness, Shankaracharya Tem...     fake        1  \n",
      "1  “The wheels of justice have started turning. A...     fake        1  \n",
      "2  “Muslim does not feature on this advert.\\n“Dai...     fake        1  \n",
      "3  “A Bangladeshi policeman threatens a child wit...     fake        1  \n",
      "4        “Muslims are being buried alive in India.\\n     fake        1  \n",
      " dataset 7 length :  2365\n",
      "        Unnamed: 0  bin_cat\n",
      "count   343.000000    343.0\n",
      "mean    870.632653      1.0\n",
      "std     698.717350      0.0\n",
      "min       0.000000      1.0\n",
      "25%     182.000000      1.0\n",
      "50%    1017.000000      1.0\n",
      "75%    1199.500000      1.0\n",
      "max    2362.000000      1.0\n",
      "    Unnamed: 0                                              title  \\\n",
      "0            0  सुप्रीम कोर्ट ने इंडिया की जगह भारत लिखने का न...   \n",
      "6            6  PM मोदी ने नहीं किया कोरोना वायरस से संक्रमित ...   \n",
      "7            7  झारखंड के एक गांव में पिछले साल मिली राम और सी...   \n",
      "8            8  दिल्ली के लोकनायक जयप्रकाश नारायण अस्पताल के न...   \n",
      "12          12  वीडियो का वाराणसी से कोई संबंध नहीं है, फर्जी ...   \n",
      "\n",
      "                                                 link          date  \\\n",
      "0   https://www.vishvasnews.com/politics/fact-chec...  June 4, 2020   \n",
      "6   https://www.vishvasnews.com/politics/fact-chec...  June 2, 2020   \n",
      "7   https://www.vishvasnews.com/politics/fact-chec...  June 1, 2020   \n",
      "8   https://www.vishvasnews.com/politics/fact-chec...  June 1, 2020   \n",
      "12  https://www.vishvasnews.com/politics/fact-chec...  May 28, 2020   \n",
      "\n",
      "                                                 text  \\\n",
      "0   15 जून से भारत का नाम हर भाषा मे सिर्फ भारत रह...   \n",
      "6    1 crore treated. 👏😅  Last I checked even the ...   \n",
      "7   Ayodhya Mein Khudai karte samay Ram Lakshman J...   \n",
      "8   ये दिल्ली के हालात है। लाशें इतनी ज्यादा है कि...   \n",
      "12  भारतीय मुस्लिम खतरे में हैं। यह चौंकाने वाला व...   \n",
      "\n",
      "                    archive category  bin_cat  \n",
      "0   http://archive.md/Xxwac     fake        1  \n",
      "6   http://archive.md/Ja7gM     fake        1  \n",
      "7   http://archive.md/keYQJ     fake        1  \n",
      "8   http://archive.md/VK5gf     fake        1  \n",
      "12  http://archive.md/JiXKt     fake        1  \n",
      "Whole dataset length :  16020\n"
     ]
    }
   ],
   "source": [
    "def data_prepare(file_path=\"/home/shiva/Desktop/datasets/\"):\n",
    "    f1=os.path.join(file_path,\"chatting1.csv\")\n",
    "    f2=os.path.join(file_path,\"chatting2.csv\")\n",
    "    f3=os.path.join(file_path,\"chatting3.csv\")\n",
    "    f4=os.path.join(file_path,\"fake_data.csv\")\n",
    "    f5=os.path.join(file_path,\"GeneralNewsArticle.csv\")\n",
    "    f6=os.path.join(file_path,\"fake_message_dataset.csv\")\n",
    "    f7=os.path.join(file_path,\"vishwas.csv\")\n",
    "    df1=pd.read_csv(f1)\n",
    "    df2=pd.read_csv(f2)\n",
    "    df3=pd.read_csv(f3)\n",
    "    df4=pd.read_csv(f4)\n",
    "    df5=pd.read_csv(f5,nrows=7000)\n",
    "    df6=pd.read_csv(f6)\n",
    "    df7=pd.read_csv(f7)\n",
    "#     Preparing df1 \n",
    "    print(\" dataset  length : \",len(df1))\n",
    "    df1['bin_cat']=df1.category.map({'chatting':0})\n",
    "    df1.dropna(inplace=True)\n",
    "    print(df1.describe())\n",
    "    print(df1.head())\n",
    "    # Preparing df2 \n",
    "    print(\" dataset 2 length : \",len(df2))\n",
    "    df2['bin_cat']=df2.category.map({'chatting':0})\n",
    "    df2.dropna(inplace=True)\n",
    "    print(df2.describe())\n",
    "    print(df2.head())\n",
    "    # Preparing df3 \n",
    "    print(\" dataset 3 length : \",len(df3))\n",
    "    df3['bin_cat']=df3.category.map({'chatting':0})\n",
    "    df3.rename(columns={'txt':'text'},inplace=True)\n",
    "    df3.dropna(inplace=True)\n",
    "    print(df3.describe())\n",
    "    print(df3.head())\n",
    "    # Preparing df4 \n",
    "    print(\" dataset 4 length : \",len(df4))\n",
    "    df4['bin_cat']=df4.category.map({'fake':1})\n",
    "    df4.dropna(inplace=True)\n",
    "    print(df4.describe())\n",
    "    print(df4.head())\n",
    "    # Preparing df5 \n",
    "    print(\" dataset 5 length : \",len(df5))\n",
    "    df5['bin_cat']=df5.category.map({'General News':0})\n",
    "    df5.dropna(inplace=True)\n",
    "    print(df5.describe())\n",
    "    print(df5.head())\n",
    "    # Preparing df6 \n",
    "    print(\" dataset 6 length : \",len(df6))\n",
    "    df6['bin_cat']=df6.category.map({'fake':1})\n",
    "    df6.dropna(inplace=True)\n",
    "    print(df6.describe())\n",
    "    print(df6.head())\n",
    "    # Preparing df7 \n",
    "    print(\" dataset 7 length : \",len(df7))\n",
    "    df7['bin_cat']=df7.category.map({'fake':1})\n",
    "    df7.dropna(inplace=True)\n",
    "    print(df7.describe())\n",
    "    print(df7.head())\n",
    "    # Organizing datasets\n",
    "    c1,c2,c3='title','text','bin_cat'\n",
    "    objs=[df1[[c2,c3]],df2[[c2,c3]],df3[[c2,c3]],df4[[c2,c3]],df5[[c2,c3]],df6[[c2,c3]],df7[[c2,c3]]]\n",
    "    df=pd.concat(objs)\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"Whole dataset length : \",len(df))\n",
    "    return df\n",
    "df=data_prepare()\n",
    "df['category']=df['bin_cat']\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    return clean_line\n",
    "\n",
    "# Text processing (split, find token id, get embedidng)\n",
    "def transform_sentence(text, model):\n",
    "\n",
    "    \"\"\"\n",
    "    Mean embedding vector\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocess_text(raw_text, model=model):\n",
    "\n",
    "        \"\"\" \n",
    "        Excluding unknown words and get corresponding token\n",
    "        \"\"\"\n",
    "\n",
    "        raw_text = raw_text.split()\n",
    "\n",
    "        return list(filter(lambda x: x in model.vocab, raw_text))\n",
    "\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    if not tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    text_vector = np.mean(model[tokens], axis=1)\n",
    "\n",
    "    return np.array(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: get_only_chars(x))\n",
    "X_train,X_test,y_train,y_test=train_test_split(df['text'],df['bin_cat'].values,test_size=0.2,random_state=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "# from imblearn.metrics import  classification_report_imbalanced \n",
    "\n",
    "# def train(X_train, y_train):\n",
    "# #     clf = KNeighborsClassifier(n_neighbors=1,p=2,algorithm='kd_tree')\n",
    "#     clf=RandomForestClassifier(min_samples_leaf=1, min_samples_split=6, n_estimators=200, \n",
    "#                              criterion='gini', bootstrap='False', n_jobs= -1)\n",
    "#     pipe = make_pipeline_imb(TfidfVectorizer(ngram_range=(1,2),norm='l2',max_features=1000),\n",
    "#                          RandomUnderSampler(random_state=440),\n",
    "#                          clf)\n",
    "#     pipe.fit(X_train, y_train)\n",
    "#     return pipe\n",
    "\n",
    "\n",
    "# def test(pipe,X_test,y_test):\n",
    "#     y_pred = pipe.predict(X_test)\n",
    "#     probablity=pipe.predict_proba(X_test)\n",
    "#     print(\"Random forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#     print(classification_report_imbalanced(y_test, y_pred))\n",
    "#     return y_pred,probablity\n",
    "# model=train(X_train, y_train)\n",
    "# pred,prob=test(model,X_test,y_test)\n",
    "# for i,j in zip(pred,y_test.values):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oneshot Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import keras,pickle\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Concatenate, Dot, Lambda, Input,Dropout,BatchNormalization,Conv2D,MaxPooling2D,LSTM,RNN,Embedding\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>division bluesman music appellees bauer menkes...</td>\n",
       "      <td>Copyright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peter rovner ilana environment age pro agostin...</td>\n",
       "      <td>Copyright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>li gould division judge product produce san ba...</td>\n",
       "      <td>Copyright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linda dallas month russell center appellees pa...</td>\n",
       "      <td>Copyright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elmore denying federal norgle similarly plaint...</td>\n",
       "      <td>Copyright</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  division bluesman music appellees bauer menkes...  Copyright\n",
       "1  peter rovner ilana environment age pro agostin...  Copyright\n",
       "2  li gould division judge product produce san ba...  Copyright\n",
       "3  linda dallas month russell center appellees pa...  Copyright\n",
       "4  elmore denying federal norgle similarly plaint...  Copyright"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls=pd.ExcelFile('Law_Wise_Text.xlsx')\n",
    "df1=pd.read_excel(xls,'Copyright Law',usecols=['Copyright Law Text'])\n",
    "df1['label']=['Copyright' for _ in range(len(df1))]\n",
    "df1.rename(columns={'Copyright Law Text':'text'},inplace=True)\n",
    "df2=pd.read_excel(xls,'Criminal Law',usecols=['Criminal Law Text'])\n",
    "df2['label']=['Criminal' for _ in range(len(df2))]\n",
    "df2.rename(columns={'Criminal Law Text':'text'},inplace=True)\n",
    "df3=pd.read_excel(xls,'Drug Biotech Law',usecols=['Drug/Biotech Law'])\n",
    "df3['label']=['DrugBiotech' for _ in range(len(df3))]\n",
    "df3.rename(columns={'Drug/Biotech Law':'text'},inplace=True)\n",
    "df4=pd.read_excel(xls,'Patent Law',usecols=['Patent Law Text'])\n",
    "df4['label']=['Patent' for _ in range(len(df4))]\n",
    "df4.rename(columns={'Patent Law Text':'text'},inplace=True)\n",
    "df5=pd.read_excel(xls,'Employment Law',usecols=['Employement Law Text'])\n",
    "df5['label']=['Employment' for _ in range(len(df5))]\n",
    "df5.rename(columns={'Employement Law Text':'text'},inplace=True)\n",
    "df=pd.concat([df1,df2,df3,df4,df5],axis=0)\n",
    "# np.array().concat()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 119 entries, 0 to 23\n",
      "Data columns (total 2 columns):\n",
      "text     119 non-null object\n",
      "label    119 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     116\n",
      "label      5\n",
      "dtype: int64\n",
      "                                                text        label\n",
      "0  division bluesman music appellees bauer menkes...    Copyright\n",
      "0  identifies month frivolous pro anders diane in...     Criminal\n",
      "0  possession linda month amend career pro amendm...  DrugBiotech\n",
      "0  carol russell fed pro civ patent diane perjuri...       Patent\n",
      "0  general intellectual astrue rovner ilana baltz...   Employment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>bin_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>division bluesman music appellees bauer menkes...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peter rovner ilana environment age pro agostin...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>li gould division judge product produce san ba...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linda dallas month russell center appellees pa...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elmore denying federal norgle similarly plaint...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label  bin_cat\n",
       "0  division bluesman music appellees bauer menkes...  Copyright        0\n",
       "1  peter rovner ilana environment age pro agostin...  Copyright        0\n",
       "2  li gould division judge product produce san ba...  Copyright        0\n",
       "3  linda dallas month russell center appellees pa...  Copyright        0\n",
       "4  elmore denying federal norgle similarly plaint...  Copyright        0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=pickle.load(open('glove-twitter-25.pkl','rb'))\n",
    "print(df.nunique())\n",
    "print(df.drop_duplicates('label'))\n",
    "df['bin_cat']=df.label.map({'Copyright':0,'Criminal':1,'DrugBiotech':2,'Patent':3,'Employment':4})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidvectorizer \n",
    "# def tf(x):\n",
    "#     tfid=TfidfVectorizer(ngram_range=(1,2),norm='l1',max_features=784)\n",
    "#     return np.resize(tfid.fit_transform([x]).toarray(),(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>bin_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>division bluesman music appellees bauer menkes...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peter rovner ilana environment age pro agostin...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>li gould division judge product produce san ba...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linda dallas month russell center appellees pa...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elmore denying federal norgle similarly plaint...</td>\n",
       "      <td>Copyright</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label  bin_cat\n",
       "0  division bluesman music appellees bauer menkes...  Copyright        0\n",
       "1  peter rovner ilana environment age pro agostin...  Copyright        0\n",
       "2  li gould division judge product produce san ba...  Copyright        0\n",
       "3  linda dallas month russell center appellees pa...  Copyright        0\n",
       "4  elmore denying federal norgle similarly plaint...  Copyright        0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([df[df.bin_cat==0],df[df.bin_cat==1]],axis=0)\n",
    "df['text'] = df['text'].apply(lambda x: get_only_chars(x))\n",
    "X_train,X_test,y_train,y_test=train_test_split(df['text'].values,df['bin_cat'].values,test_size=0.2,random_state=420)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>bin_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>standing set inferred federal wife witness sen...</td>\n",
       "      <td>Criminal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>amendment armour leonard appealable round one ...</td>\n",
       "      <td>Criminal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>even drive november month hardy holderman lake...</td>\n",
       "      <td>Criminal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kerr hamilton craig general judge lover incorp...</td>\n",
       "      <td>Criminal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>part briley tying sachs peremptories joshua fi...</td>\n",
       "      <td>Criminal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     label  bin_cat\n",
       "19  standing set inferred federal wife witness sen...  Criminal        1\n",
       "20  amendment armour leonard appealable round one ...  Criminal        1\n",
       "21  even drive november month hardy holderman lake...  Criminal        1\n",
       "22  kerr hamilton craig general judge lover incorp...  Criminal        1\n",
       "23  part briley tying sachs peremptories joshua fi...  Criminal        1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vectorizing \n",
    "# X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
    "# X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
    "# X_train_mean = pd.DataFrame(X_train_mean)['text'].apply(pd.Series)\n",
    "# X_test_mean = pd.DataFrame(X_test_mean)['text'].apply(pd.Series)\n",
    "\n",
    "# X_train_mean_=X_train_mean.iloc[:,:100]\n",
    "# X_train_mean_=pd.concat([X_train_mean_,y_train],axis=1)\n",
    "# X_train_mean_['label']=y_train.values\n",
    "# X_train_mean_=X_train_mean_.dropna()\n",
    "# y_train=X_train_mean_.label\n",
    "# X_train_mean=X_train_mean_.iloc[:,:100]\n",
    "\n",
    "# X_test_mean_=X_test_mean.iloc[:,:100]\n",
    "# X_test_mean_=pd.concat([X_test_mean_,y_test],axis=1)\n",
    "# X_test_mean_['label']=y_test\n",
    "# X_test_mean_=X_test_mean_.dropna()\n",
    "# y_test=X_test_mean_.label\n",
    "# X_test_mean=X_test_mean_.iloc[:,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "def exponent_neg_manhattan_distance(vects):\n",
    "    x, y = vects\n",
    "    arms_difference=x-y\n",
    "    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left/right inputs\n",
    "    similarity from the inputs differences. This function is used to turned the unbounded\n",
    "    L1 distance to a similarity measure between 0 and 1\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def cosine_similarity(vects):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    x, y = vects\n",
    "    return K.sum(x*y,axis=1, keepdims=True)/K.sqrt(K.sum(x*x,axis=1, keepdims=True)*K.sum(y*y,axis=1, \n",
    "                                                                                          keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def get_siamese_model(input_shape):\n",
    "#     \"\"\"\n",
    "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "#     \"\"\"\n",
    "#     left_input = Input(input_shape)\n",
    "#     right_input = Input(input_shape)\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape))\n",
    "#     model.add(MaxPooling2D())\n",
    "#     model.add(Conv2D(128, (7,7), activation='relu'))\n",
    "#     model.add(MaxPooling2D())\n",
    "#     model.add(Conv2D(128, (4,4), activation='relu'))\n",
    "#     model.add(MaxPooling2D())\n",
    "#     model.add(Conv2D(256, (4,4), activation='relu'))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(4096, activation='sigmoid'))\n",
    "#     encoded_l = model(left_input)\n",
    "#     encoded_r = model(right_input)\n",
    "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "#     siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "#     return siamese_net\n",
    "# mod = get_siamese_model((105, 105, 1))\n",
    "# mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,) (38,)\n",
      "(38, 100) (38,) (10, 100) (10,)\n",
      "(76, 10, 10) (76, 10, 10) (76,)\n",
      "(20, 10, 10) (20, 10, 10) (20,)\n",
      "Train on 68 samples, validate on 8 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.2499 - accuracy: 0.5147 - val_loss: 0.2500 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.2491 - accuracy: 0.5882 - val_loss: 0.2501 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.2482 - accuracy: 0.5735 - val_loss: 0.2501 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.2476 - accuracy: 0.5735 - val_loss: 0.2502 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.2460 - accuracy: 0.5735 - val_loss: 0.2502 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.2457 - accuracy: 0.5735 - val_loss: 0.2503 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.2442 - accuracy: 0.5735 - val_loss: 0.2504 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.2409 - accuracy: 0.5735 - val_loss: 0.2505 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.2436 - accuracy: 0.5735 - val_loss: 0.2506 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.2380 - accuracy: 0.5735 - val_loss: 0.2506 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.2387 - accuracy: 0.5735 - val_loss: 0.2505 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.2408 - accuracy: 0.5735 - val_loss: 0.2505 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.2395 - accuracy: 0.5735 - val_loss: 0.2505 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.2415 - accuracy: 0.5735 - val_loss: 0.2505 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.2401 - accuracy: 0.5735 - val_loss: 0.2506 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.2409 - accuracy: 0.5735 - val_loss: 0.2507 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.2382 - accuracy: 0.5735 - val_loss: 0.2508 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.2385 - accuracy: 0.5735 - val_loss: 0.2509 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.2368 - accuracy: 0.5735 - val_loss: 0.2511 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.2362 - accuracy: 0.5735 - val_loss: 0.2513 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.2486 - accuracy: 0.5735 - val_loss: 0.2512 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.2323 - accuracy: 0.5735 - val_loss: 0.2511 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.2385 - accuracy: 0.5735 - val_loss: 0.2511 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.2375 - accuracy: 0.5735 - val_loss: 0.2510 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.2353 - accuracy: 0.5735 - val_loss: 0.2510 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.2379 - accuracy: 0.5735 - val_loss: 0.2510 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.2339 - accuracy: 0.5735 - val_loss: 0.2511 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.2376 - accuracy: 0.5735 - val_loss: 0.2512 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.2361 - accuracy: 0.5735 - val_loss: 0.2513 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.2327 - accuracy: 0.5735 - val_loss: 0.2514 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.2372 - accuracy: 0.5735 - val_loss: 0.2514 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.2338 - accuracy: 0.5735 - val_loss: 0.2516 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.2314 - accuracy: 0.5735 - val_loss: 0.2519 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.2362 - accuracy: 0.5735 - val_loss: 0.2520 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.2281 - accuracy: 0.5735 - val_loss: 0.2521 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.2372 - accuracy: 0.5735 - val_loss: 0.2519 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.2320 - accuracy: 0.5735 - val_loss: 0.2517 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.2357 - accuracy: 0.5735 - val_loss: 0.2516 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.2363 - accuracy: 0.5735 - val_loss: 0.2517 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.2367 - accuracy: 0.5735 - val_loss: 0.2518 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.2305 - accuracy: 0.5735 - val_loss: 0.2521 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.2334 - accuracy: 0.5735 - val_loss: 0.2523 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.2326 - accuracy: 0.5735 - val_loss: 0.2521 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.2341 - accuracy: 0.5735 - val_loss: 0.2520 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.2322 - accuracy: 0.5735 - val_loss: 0.2522 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.2313 - accuracy: 0.5735 - val_loss: 0.2525 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.2311 - accuracy: 0.5735 - val_loss: 0.2532 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.2297 - accuracy: 0.5735 - val_loss: 0.2541 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.2307 - accuracy: 0.5735 - val_loss: 0.2550 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.2317 - accuracy: 0.5735 - val_loss: 0.2547 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f1984f319b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2vec\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "def doc2vec(X_train):\n",
    "    document_tagged = []\n",
    "    tagged_count = 0\n",
    "    for line in X_train:\n",
    "        document_tagged.append(gensim.models.doc2vec.TaggedDocument(line,[tagged_count]))\n",
    "        tagged_count +=1 \n",
    "    d2v = Doc2Vec(document_tagged)\n",
    "    d2v.train(document_tagged,epochs=d2v.epochs,total_examples=d2v.corpus_count)\n",
    "    return d2v.docvecs.vectors_docs\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "X_train=doc2vec(X_train)\n",
    "X_test=doc2vec(X_test)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "### GENERATE PAIR SAMPLES FOR SIAMESE NETWORK ###\n",
    "\n",
    "def pairs(X_train,Y_train):\n",
    "    sequence_length=10\n",
    "    left_input = []\n",
    "    right_input = []\n",
    "    targets = []\n",
    "    pairs = 2\n",
    "    features=['text']\n",
    "    for i in range(len(Y_train)):\n",
    "        for _ in range(pairs):\n",
    "            compare_to = i\n",
    "            while compare_to == i:\n",
    "                compare_to = random.randint(0,len(Y_train)-1)\n",
    "            left_input.append(X_train[i])\n",
    "            right_input.append(X_train[compare_to])\n",
    "            if Y_train[i] == Y_train[compare_to]: # They are the same\n",
    "                targets.append(1.)\n",
    "            else:# Not the same\n",
    "                targets.append(0.)\n",
    "\n",
    "    left_input = np.asarray(left_input).reshape(-1, sequence_length, 10)# len(features)\n",
    "    right_input = np.asarray(right_input).reshape(-1, sequence_length, 10)# len(features)\n",
    "    targets = np.asarray(targets)\n",
    "    print(left_input.shape, right_input.shape, targets.shape)\n",
    "    return left_input,right_input,targets\n",
    "left_input,right_input,targets=pairs(X_train,y_train)\n",
    "left_input_,right_input_,targets_=pairs(X_test,y_test)\n",
    "\n",
    "### DEFINE SIAMESE NETWORK ARCHITECTURE ###\n",
    "\n",
    "test_size = 0.1\n",
    "def SiamesNet(shape=(10,10)):\n",
    "    \n",
    "    left_input = Input(shape)\n",
    "    right_input = Input(shape)\n",
    "\n",
    "    lstmnet = Sequential([\n",
    "        BatchNormalization(),\n",
    "        LSTM(128, activation='relu', return_sequences=True, \n",
    "             input_shape=shape),\n",
    "        LSTM(32, activation='relu')\n",
    "    ])\n",
    "\n",
    "    encoded_l = lstmnet(left_input)\n",
    "    encoded_r = lstmnet(right_input)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensor: K.abs(tensor[0] - tensor[1]))\n",
    "\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    drop = Dropout(0.2)(L1_distance)\n",
    "    prediction = Dense(1,activation='sigmoid')(drop)\n",
    "    model = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "### FIT SIAMESE NETWORK ###\n",
    "\n",
    "siamese_net = SiamesNet()\n",
    "siamese_net.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "siamese_net.fit([left_input, right_input], targets, batch_size=32, epochs=50, \n",
    "                validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc of f1,f2 :  313 1187\n",
      "0 3\n",
      "probablity of verification of matching  0\n"
     ]
    }
   ],
   "source": [
    "def check(num):\n",
    "    if num>0.7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "n1=np.random.randint(0,1600)\n",
    "n2=np.random.randint(0,1600)\n",
    "print('loc of f1,f2 : ',n1,n2)\n",
    "x1 = np.resize(transform_sentence(df['text'].values[n1], model2),(1,10,10))\n",
    "x2 = np.resize(transform_sentence(df['text'].values[n2], model2),(1,10,10))\n",
    "print(df['bin_cat'].values[n1],df['bin_cat'].values[n2])\n",
    "print('probablity of verification of matching ',check(model.predict([x1,x2])[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
